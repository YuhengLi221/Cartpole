{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This program could not achieve the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "env = gym.make('CartPole-v1').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "           \n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    " \n",
    "                \n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):   \n",
    "        x=F.relu(self.fc1(x))\n",
    "        \n",
    "             \n",
    "        probs=F.softmax(self.fc2(x), dim=1)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \n",
    "    R=0\n",
    "    Re_rewards=[] #rectified reward\n",
    "    Re_log = torch.stack(policy.log_probs)\n",
    "    \n",
    "    for r in policy.rewards[::-1]:\n",
    "        R=R*0.9+r\n",
    "        Re_rewards.insert(0,R)        \n",
    "    \n",
    " \n",
    "          \n",
    "    Re_rewards=torch.tensor(Re_rewards)\n",
    "#     Re_rewards-=Re_rewards.mean()\n",
    "#     Re_rewards/=Re_rewards.std()\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    loss= -( Re_rewards*Re_log).sum()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes):\n",
    "   \n",
    "    scores=[]\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        state = env.reset() # reset environment        \n",
    "        \n",
    "        for t in range(1000):\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "             \n",
    "            if done :                \n",
    "                reward = -1\n",
    "                \n",
    "            policy.rewards.append(reward)\n",
    "            \n",
    "            if done:                         \n",
    "                scores.append(t)\n",
    "                break\n",
    "        \n",
    "        update()\n",
    "        policy.rewards = []\n",
    "        policy.log_probs = []\n",
    "        \n",
    "    print(scores)\n",
    "    print('the average score of last consecutive 100 episodes is',sum(scores[300:400])/100)        \n",
    "    print('from many times observations, this model could not encounter 100 consecutive episodes with an average score of above 195')\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 12, 12, 19, 10, 9, 14, 39, 16, 10, 24, 13, 14, 13, 33, 11, 19, 15, 28, 15, 28, 14, 17, 32, 18, 42, 26, 11, 22, 10, 26, 25, 24, 38, 23, 39, 27, 34, 36, 35, 51, 57, 11, 69, 31, 45, 42, 14, 42, 30, 32, 29, 17, 63, 43, 87, 29, 82, 57, 23, 37, 49, 57, 49, 37, 53, 72, 29, 35, 87, 51, 91, 52, 34, 45, 65, 93, 41, 51, 37, 41, 93, 21, 62, 75, 30, 30, 49, 15, 24, 51, 47, 61, 30, 50, 74, 49, 58, 39, 50, 147, 38, 61, 49, 46, 57, 30, 55, 55, 81, 33, 59, 46, 21, 94, 62, 56, 56, 48, 54, 38, 63, 65, 89, 65, 51, 125, 24, 47, 44, 45, 50, 35, 46, 47, 35, 29, 44, 32, 35, 39, 46, 46, 23, 27, 54, 23, 29, 33, 33, 21, 45, 18, 28, 20, 32, 30, 64, 38, 24, 41, 50, 39, 34, 83, 38, 34, 27, 37, 50, 58, 23, 44, 30, 94, 56, 29, 88, 46, 45, 31, 32, 42, 32, 35, 45, 43, 48, 30, 33, 48, 51, 36, 33, 26, 38, 39, 32, 32, 31, 31, 20, 29, 28, 22, 42, 32, 28, 20, 49, 30, 41, 35, 31, 19, 37, 33, 20, 90, 38, 23, 53, 34, 65, 58, 29, 58, 49, 65, 50, 48, 38, 55, 89, 36, 83, 29, 67, 60, 42, 51, 85, 66, 55, 44, 75, 55, 70, 98, 45, 109, 97, 45, 59, 90, 50, 56, 81, 60, 34, 54, 120, 148, 52, 49, 66, 57, 100, 49, 49, 40, 44, 34, 29, 39, 42, 47, 111, 34, 61, 57, 58, 28, 62, 58, 36, 45, 28, 41, 38, 47, 32, 52, 36, 73, 27, 54, 54, 26, 28, 109, 36, 26, 37, 93, 32, 34, 51, 37, 106, 18, 77, 56, 109, 113, 43, 53, 68, 95, 105, 64, 109, 58, 48, 55, 47, 95, 43, 58, 116, 29, 42, 77, 48, 63, 53, 46, 42, 45, 49, 76, 65, 81, 104, 31, 26, 99, 35, 24, 35, 64, 64, 65, 35, 65, 47, 54, 62, 88, 126, 100, 64, 61, 63, 44, 54, 66, 39, 72, 87, 90, 96, 109, 69, 72, 81, 67, 54, 43, 92, 61, 75, 62, 46, 54, 50, 67, 76, 100, 40, 66, 36, 80, 59, 52, 52, 44, 83, 48, 48]\n",
      "the average score of last consecutive 100 episodes is 63.53\n",
      "from many times observations, this model could not encounter 100 consecutive episodes with an average score of above 195\n"
     ]
    }
   ],
   "source": [
    "train(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
